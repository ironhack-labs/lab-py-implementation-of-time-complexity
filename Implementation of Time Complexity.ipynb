{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Implementation of Time Complexity on Python Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What Is Time Complexity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Time complexity is a computational concept that describes the amount of time an algorithm takes to complete as a function of the length of the input. It provides a way to evaluate the efficiency of an algorithm by expressing how its execution time grows relative to the size of the input data. Time complexity is typically represented using Big O notation, which classifies algorithms according to their worst-case or upper-bound performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importance of Time Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Time complexity is crucial in real-world applications because it helps developers predict how an algorithm will perform as the size of the input increases. In environments where performance and speed are critical, understanding time complexity allows developers to choose algorithms that will execute efficiently within the constraints of available resources. \n",
    "\n",
    "Selecting an appropriate algorithm based on its time complexity can prevent applications from becoming sluggish or unresponsive. For instance, if an algorithm takes too long to execute, it can lead to poor user experiences or even system failures, particularly when processing large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Types of Time Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we understand the basics of time complexity, let's explore common types and their implications:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant Time: O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Constant time complexity indicates that an algorithm's execution time remains fixed regardless of the input size. This means that no matter how large the dataset grows, the performance remains unchanged. \n",
    "\n",
    "For example, consider a function that calculates the distance between two points:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(p, q):\n",
    "    dx = (p[0] - q[0]) ** 2  # 1 Time\n",
    "    dy = (p[1] - q[1]) ** 2  # 1 Time\n",
    "    return (dx + dy) ** 0.5   # 1 Time\n",
    "\n",
    "### NOTE I do not understand! in the code there is a square ? actually two!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this case, regardless of the coordinates provided, the number of operations remains constant at three instructions. Thus, we express its time complexity as O(1). \n",
    "\n",
    "Interestingly, functions can exhibit constant time complexity even when processing large datasets. For instance, calling `len()` on a list is O(1) because Python maintains an internal count of the list's size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Linear Time: O(N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear time complexity arises when an algorithm's execution time increases directly in proportion to the size of the input data. In other words, if you double the amount of data, you can expect the execution time to also double.\n",
    "\n",
    "A classic example is finding the minimum value in a list:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum(lst):\n",
    "    min_value = lst[0]  # 1 instruction\n",
    "    for i in range(1, len(lst)): # N times\n",
    "        min_value = min(min_value, lst[i])  # executed len(lst) - 1 times\n",
    "    return min_value  # 1 instruction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If there are N elements in `lst`, then this function executes approximately N operations plus a couple of constant-time instructions. Therefore, we express its time complexity as O(N).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Log-linear Time: O(N log N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logarithmic time complexity occurs when an algorithm's execution time grows logarithmically as the input size increases. This often happens in algorithms that divide their problem space in half with each iteration.\n",
    "\n",
    "Log-linear time complexity is common in efficient sorting algorithms like merge sort and quicksort. These algorithms achieve better performance than quadratic ones by combining linear and logarithmic operations.\n",
    "\n",
    "A well-known example is binary search:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this implementation, each guess eliminates half of the remaining elements from consideration. The maximum number of guesses required corresponds to log base 2 of N (i.e., log₂(N)). Thus, we express its time complexity as O(log N). Logarithmic complexities are highly desirable due to their efficiency; even for large inputs like one billion items, only about thirty operations are needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Quadratic Space Complexity: O(n²)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Certain algorithms may require a two-dimensional array or matrix based on their inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(n):\n",
    "    matrix = [[0] * n for _ in range(n)]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here, if `n` is 5, a 5x5 matrix is created, resulting in a space complexity of O(n²).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Polynomial Time Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Polynomial time complexities occur when an algorithm's running time grows polynomially with respect to the input size $ n $. This means that if you were to graph such functions against $ n $, they would yield curves that rise steeply as $ n $ increases. Common forms include $ O(n^2) $, $ O(n^3) $, etc.\n",
    "\n",
    "#### Example of Polynomial Time Complexity: O(n²)\n",
    "\n",
    "A classic example of polynomial time complexity is bubble sort:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(arr):\n",
    "    n = len(arr)                                    # 1 time\n",
    "    for i in range(n):                              # N Times\n",
    "        for j in range(0, n-i-1):                   # N times\n",
    "            if arr[j] > arr[j+1]:                   # N * N times\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, we have two nested loops iterating over $ n $. The outer loop runs $ n $ times while the inner loop runs up to $ n-i-1 $, leading us to conclude that bubble sort has a time complexity of $ O(n^2) $.\n",
    "\n",
    "The implications of using polynomial time algorithms can be significant; while they may be manageable for small inputs (e.g., sorting a few hundred elements), they become impractical as data sizes grow into thousands or millions due to their rapidly increasing execution times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cubic Time: O(N³)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cubic time complexity arises when an algorithm involves three nested loops over the data set. A practical example is matrix multiplication:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_mul(A, B):\n",
    "    n = len(A)  # 1 instruction\n",
    "    res = [[0 for _ in range(n)] for _ in range(n)]  # N² instructions\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(n):\n",
    "                res[i][j] += A[i][k] * B[k][j]  # executed N×N×N = N³ times\n",
    "\n",
    "    return res  # 1 instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here, each entry in the resulting matrix requires summing products across rows and columns—resulting in a total execution count of O(N³). Cubic algorithms scale poorly; even modest increases in input size can lead to dramatic increases in computation time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exponential Time Complexity: O(2^N) and O(N!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Exponential time complexities arise when algorithms must explore all possible solutions or combinations within a dataset. This growth rate is represented as O(2^N) or O(N!).\n",
    "\n",
    "Consider a delivery route optimization problem where all possible delivery orders must be evaluated:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def optimize_route(locations):\n",
    "    minimum_distance = float(\"inf\")  # 1 instruction\n",
    "    best_order = None  # 1 instruction\n",
    "\n",
    "    for order in itertools.permutations(locations):  # N! iterations\n",
    "        distance = sum(dist(order[i], order[i + 1]) for i in range(len(order) - 1))\n",
    "\n",
    "        if distance < minimum_distance:\n",
    "            minimum_distance = distance\n",
    "            best_order = order\n",
    "\n",
    "    return best_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, evaluating every permutation leads us to conclude that its overall complexity is O(N!). In this case, if you have $ n $ items (e.g., letters or numbers), the number of permutations generated will be $ n! $. For instance:\n",
    "\n",
    "- For $n = 3$: The permutations are $3! = 6 $.\n",
    "- For $n = 4$: The permutations increase dramatically to $4! = 24 $.\n",
    "  \n",
    "This rapid growth illustrates why factorial complexities are often impractical beyond small values; even at $ n = 13 $, there are over one billion possible arrangements!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example of Exponential Time Complexity: O(2ⁿ)\n",
    "\n",
    "Consider a recursive solution for generating all subsets of a set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsets(s):\n",
    "    if not s:\n",
    "        return [[]]\n",
    "\n",
    "    subsets = generate_subsets(s[1:])\n",
    "    return subsets + [[s[0]] + subset for subset in subsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, each element can either be included or excluded from a subset. Thus, for each element added into consideration (i.e., each recursive call), we double our possibilities—leading us to conclude that this algorithm operates at $ O(2^n) $.\n",
    "\n",
    "Exponential algorithms are typically infeasible beyond very small inputs due to their rapid growth rates; even small values like $ n = 20 $ result in over a million operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Comparing Different Types of Time Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Understanding how different types of complexities scale helps us make informed decisions when selecting algorithms:\n",
    "\n",
    "- **Constant (O(1))**: Fastest and most efficient; ideal for fixed operations.\n",
    "- **Logarithmic (O(log N))**: Highly efficient; preferred for searching large datasets.\n",
    "- **Linear (O(N))**: Directly proportional; manageable for moderate-sized datasets.\n",
    "- **Log-linear (O(N log N))**: Efficient sorting; balances performance with scalability.\n",
    "- **Quadratic (O(N²))**: Inefficient for large datasets; suitable only for small inputs.\n",
    "- **Cubic (O(N³))**: Slow growth; impractical beyond small datasets.\n",
    "- **Exponential (O(2^N), O(N!))**: Extremely slow; only feasible for very small inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Understanding time complexity is essential when designing efficient algorithms capable of handling varying data sizes effectively. By analyzing these complexities thoughtfully and choosing appropriate algorithms based on specific requirements and constraints, developers can create robust applications capable of addressing real-world challenges efficiently.\n",
    "\n",
    "As you continue your exploration into algorithm design and analysis across various programming languages—including Python—keep these principles regarding different types of complexities at hand as you strive towards writing efficient code tailored specifically for your challenges while being mindful not only of execution speed but also resource consumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend Resource: https://www.savemyexams.com/a-level/computer-science/ocr/17/revision-notes/8-algorithms/8-1-algorithms/big-o-notation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
